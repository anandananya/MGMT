{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-04T20:22:44.964819Z",
     "iopub.status.busy": "2022-05-04T20:22:44.964438Z",
     "iopub.status.idle": "2022-05-04T20:23:20.1754Z",
     "shell.execute_reply": "2022-05-04T20:23:20.174269Z",
     "shell.execute_reply.started": "2022-05-04T20:22:44.964728Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install scikit-learn\n",
    "!pip3 install imageio\n",
    "!pip3 install IPython\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import tensorflow.python.keras.backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from numpy import save, load\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:20.177867Z",
     "iopub.status.busy": "2022-05-04T20:23:20.177612Z",
     "iopub.status.idle": "2022-05-04T20:23:24.752262Z",
     "shell.execute_reply": "2022-05-04T20:23:24.7514Z",
     "shell.execute_reply.started": "2022-05-04T20:23:20.177837Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# tensorflow.keras.backend.set_floatx('float16')\n",
    "\n",
    "PATCH_SIZE_X=32\n",
    "PATCH_SIZE_Y=32\n",
    "PATCH_SIZE_Z=32\n",
    "\n",
    "SampleX = []\n",
    "SampleY = []\n",
    "LabelY = []\n",
    "\n",
    "category_1_count = 0\n",
    "category_2_count = 0\n",
    "category_3_count = 0\n",
    "category_4_count = 0\n",
    "total_count = 0\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    weights = K.variable(weights)        \n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss    \n",
    "    return loss\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + K.epsilon()) / (\n",
    "                K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())\n",
    "                    \n",
    "def soft_dice_loss(y_true, y_pred):\n",
    "    print(\"y_true\")\n",
    "    print(y_true.shape)\n",
    "    print(\"y_pred\")\n",
    "    print(y_pred.shape)\n",
    "    # dice_loss = 1 - (1/4 * (dice_coef(y_true[0][0],y_pred[0][0])+dice_coef(y_true[0][1],y_pred[0][1])+dice_coef(y_true[0][2],y_pred[0][2])+dice_coef(y_true[0][3],y_pred[0][3])))\n",
    "    dice_loss = 1 - (1/3 * (dice_coef(y_true[0][0],y_pred[0][0])+dice_coef(y_true[0][1],y_pred[0][1])+dice_coef(y_true[0][2],y_pred[0][2])))\n",
    "    return dice_loss\n",
    "\n",
    "# datasets=['model1batch1/']\n",
    "# datasets=['model1batch2/'] - best \n",
    "# datasets=['model1batch3/']\n",
    "# datasets=['model1batch4/'] \n",
    "# datasets=['modl1batch5/'] - worst\n",
    "# datasets=['model1batch6/']\n",
    "\n",
    "# datasets=['model1batch7/']\n",
    "# datasets = ['model1batch8/']\n",
    "# datasets = ['model1batch9/']\n",
    "# datasets=['model1batch10/']\n",
    "# datasets=['model1batch11/']\n",
    "\n",
    "datasets = []\n",
    "for dataset in datasets:\n",
    "    for patch_file in os.listdir('../input/'+dataset):\n",
    "        #filling in my input layer\n",
    "        if \"nifti\" in patch_file:\n",
    "            image = np.load('../input/'+dataset+patch_file)\n",
    "            # train_x ~ empty 4 channel input tensor\n",
    "            train_x = np.empty((4,PATCH_SIZE_X,PATCH_SIZE_Y,PATCH_SIZE_Z))\n",
    "            # patient ID and patch number\n",
    "            patient_id = patch_file.split('-')[1]\n",
    "            patch_num = patch_file.split('-')[2].split(\".\")[0]\n",
    "            ngtdm = 0\n",
    "            \n",
    "            # we've gone from patch creation --> numpy files --> train_x\n",
    "            for x in range(0,PATCH_SIZE_X):\n",
    "                for y in range(0,PATCH_SIZE_Y):\n",
    "                    for z in range(0,PATCH_SIZE_Z):\n",
    "                        train_x[0][x][y][z] = np.uint8(image[0][x][y][z])\n",
    "                        train_x[1][x][y][z] = np.uint8(image[1][x][y][z])\n",
    "                        train_x[2][x][y][z] = image[2][x][y][z]\n",
    "                        train_x[3][x][y][z] = image[3][x][y][z]\n",
    "                        total_count = total_count + 1\n",
    "            SampleX.append(train_x)\n",
    "            # print('image shape')\n",
    "            # print(image.shape)\n",
    "        \n",
    "        if \"label\" in patch_file:\n",
    "            label = np.load('../input/'+dataset+patch_file)\n",
    "            \n",
    "            # create label dataset ~ 1x32x32x32\n",
    "            train_y = np.empty((4,PATCH_SIZE_X,PATCH_SIZE_Y,PATCH_SIZE_Z))\n",
    "            patient_id = patch_file.split('-')[1]\n",
    "            patch_num = patch_file.split('-')[2].split(\".\")[0]\n",
    "            for x in range(0,PATCH_SIZE_X):\n",
    "                for y in range(0,PATCH_SIZE_Y):\n",
    "                    for z in range(0,PATCH_SIZE_Z):\n",
    "                        if label[x][y][z] == 1:\n",
    "                            train_y[0][x][y][z] = 1\n",
    "                            train_y[1][x][y][z] = 0\n",
    "                            train_y[2][x][y][z] = 0\n",
    "                            train_y[3][x][y][z] = 0\n",
    "                            category_1_count = category_1_count + 1\n",
    "                        elif label[x][y][z] == 2:\n",
    "                            # print(\"category (%d,%d,%d) 1\" % (x,y,z))\n",
    "                            train_y[0][x][y][z] = 0\n",
    "                            train_y[1][x][y][z] = 1\n",
    "                            train_y[2][x][y][z] = 0\n",
    "                            train_y[3][x][y][z] = 0\n",
    "                            category_2_count = category_2_count + 1\n",
    "                        elif label[x][y][z] == 3:\n",
    "                            # print(\"category (%d,%d,%d) 3\" % (x,y,z))\n",
    "                            train_y[0][x][y][z] = 0\n",
    "                            train_y[1][x][y][z] = 0\n",
    "                            train_y[2][x][y][z] = 1\n",
    "                            train_y[3][x][y][z] = 0\n",
    "                            category_3_count = category_3_count + 1\n",
    "                        else:\n",
    "                            # print(\"category (%d,%d,%d) 2\" % (x,y,z))\n",
    "                            train_y[0][x][y][z] = 0\n",
    "                            train_y[1][x][y][z] = 0\n",
    "                            train_y[2][x][y][z] = 0\n",
    "                            train_y[3][x][y][z] = 1\n",
    "                            category_4_count = category_4_count + 1\n",
    "            # append all patches into a patch array (list of patches)\n",
    "            # sampleY ~ for 5 fold cross validation\n",
    "            LabelY.append(label)\n",
    "            SampleY.append(train_y)\n",
    "\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "model=tensorflow.keras.models.load_model('../input/d/datasets/sokrishn/segmentationmodel/segmentation.model',custom_objects={'soft_dice_loss':soft_dice_loss})\n",
    "model.save('./segmentation.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:24.754184Z",
     "iopub.status.busy": "2022-05-04T20:23:24.753511Z",
     "iopub.status.idle": "2022-05-04T20:23:24.762525Z",
     "shell.execute_reply": "2022-05-04T20:23:24.761341Z",
     "shell.execute_reply.started": "2022-05-04T20:23:24.754148Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, UpSampling3D, Activation, BatchNormalization, PReLU, Conv3DTranspose\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "keras.backend.set_image_data_format(\"channels_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:24.765799Z",
     "iopub.status.busy": "2022-05-04T20:23:24.765428Z",
     "iopub.status.idle": "2022-05-04T20:23:24.784598Z",
     "shell.execute_reply": "2022-05-04T20:23:24.783613Z",
     "shell.execute_reply.started": "2022-05-04T20:23:24.765754Z"
    }
   },
   "outputs": [],
   "source": [
    "# first layer\n",
    "input_layer = Input(shape=(4,32,32,32))\n",
    "input_layer\n",
    "\n",
    "# first down convolution \n",
    "down_depth_0_layer_0 = keras.layers.Conv3D(filters=32, \n",
    "                              kernel_size=(3,3,3),\n",
    "                              padding='same',\n",
    "                              strides=(1,1,1),\n",
    "                              data_format='channels_first',\n",
    "                              input_shape=(4,32,32,32)\n",
    "                              )(input_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:24.786581Z",
     "iopub.status.busy": "2022-05-04T20:23:24.785866Z",
     "iopub.status.idle": "2022-05-04T20:23:24.79785Z",
     "shell.execute_reply": "2022-05-04T20:23:24.797161Z",
     "shell.execute_reply.started": "2022-05-04T20:23:24.786521Z"
    }
   },
   "outputs": [],
   "source": [
    "# first down convolution - activation layer 1\n",
    "down_depth_0_layer_0 = Activation('relu')(down_depth_0_layer_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:24.799867Z",
     "iopub.status.busy": "2022-05-04T20:23:24.79912Z",
     "iopub.status.idle": "2022-05-04T20:23:24.8186Z",
     "shell.execute_reply": "2022-05-04T20:23:24.817739Z",
     "shell.execute_reply.started": "2022-05-04T20:23:24.799831Z"
    }
   },
   "outputs": [],
   "source": [
    "# first down convolution - activation layer 1\n",
    "# Create a Conv3D layer with 64 filters and add relu activation\n",
    "down_depth_0_layer_1 = Conv3D(filters=64, \n",
    "                kernel_size=(3,3,3),\n",
    "                padding='same',\n",
    "                strides=(1,1,1)\n",
    "               )(down_depth_0_layer_0)\n",
    "down_depth_0_layer_1 = Activation('relu')(down_depth_0_layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:24.82066Z",
     "iopub.status.busy": "2022-05-04T20:23:24.820307Z",
     "iopub.status.idle": "2022-05-04T20:23:24.833193Z",
     "shell.execute_reply": "2022-05-04T20:23:24.832077Z",
     "shell.execute_reply.started": "2022-05-04T20:23:24.820611Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a max pooling layer\n",
    "down_depth_0_layer_pool = MaxPooling3D(pool_size=(2,2,2),data_format='channels_first')(down_depth_0_layer_1)\n",
    "down_depth_0_layer_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:24.835456Z",
     "iopub.status.busy": "2022-05-04T20:23:24.834889Z",
     "iopub.status.idle": "2022-05-04T20:23:24.847424Z",
     "shell.execute_reply": "2022-05-04T20:23:24.846641Z",
     "shell.execute_reply.started": "2022-05-04T20:23:24.835411Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a Conv3D layer to your network with relu activation\n",
    "down_depth_1_layer_0 = Conv3D(filters=64, \n",
    "                kernel_size=(3,3,3),\n",
    "                padding='same',\n",
    "                strides=(1,1,1)\n",
    "               )(down_depth_0_layer_pool)\n",
    "down_depth_1_layer_0 = Activation('relu')(down_depth_1_layer_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:24.849675Z",
     "iopub.status.busy": "2022-05-04T20:23:24.849326Z",
     "iopub.status.idle": "2022-05-04T20:23:24.861966Z",
     "shell.execute_reply": "2022-05-04T20:23:24.860769Z",
     "shell.execute_reply.started": "2022-05-04T20:23:24.849633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add another Conv3D with 128 filters to your network.\n",
    "down_depth_1_layer_1 = Conv3D(filters=128, \n",
    "                kernel_size=(3,3,3),\n",
    "                padding='same',\n",
    "                strides=(1,1,1)\n",
    "               )(down_depth_1_layer_0)\n",
    "down_depth_1_layer_1 = Activation('relu')(down_depth_1_layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:24.864939Z",
     "iopub.status.busy": "2022-05-04T20:23:24.86467Z",
     "iopub.status.idle": "2022-05-04T20:23:24.8745Z",
     "shell.execute_reply": "2022-05-04T20:23:24.873911Z",
     "shell.execute_reply.started": "2022-05-04T20:23:24.864909Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add an upsampling operation to your network\n",
    "up_depth_0_layer_0 = UpSampling3D(size=(2,2,2))(down_depth_1_layer_1)\n",
    "up_depth_0_layer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:24.876641Z",
     "iopub.status.busy": "2022-05-04T20:23:24.875632Z",
     "iopub.status.idle": "2022-05-04T20:23:24.890755Z",
     "shell.execute_reply": "2022-05-04T20:23:24.889648Z",
     "shell.execute_reply.started": "2022-05-04T20:23:24.876514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the shape of layers to concatenate\n",
    "print(up_depth_0_layer_0)\n",
    "print(down_depth_0_layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:24.893022Z",
     "iopub.status.busy": "2022-05-04T20:23:24.892678Z",
     "iopub.status.idle": "2022-05-04T20:23:24.90504Z",
     "shell.execute_reply": "2022-05-04T20:23:24.903775Z",
     "shell.execute_reply.started": "2022-05-04T20:23:24.892975Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a concatenation along axis 1\n",
    "up_depth_1_concat = concatenate([up_depth_0_layer_0,\n",
    "                                 down_depth_0_layer_1],\n",
    "                                axis=1)\n",
    "up_depth_1_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:24.908761Z",
     "iopub.status.busy": "2022-05-04T20:23:24.908333Z",
     "iopub.status.idle": "2022-05-04T20:23:24.921445Z",
     "shell.execute_reply": "2022-05-04T20:23:24.920853Z",
     "shell.execute_reply.started": "2022-05-04T20:23:24.908696Z"
    }
   },
   "outputs": [],
   "source": [
    "down_depth_0_layer_1\n",
    "# Add a Conv3D up-convolution with 64 filters to your network\n",
    "up_depth_1_layer_1 = keras.layers.Conv3D(filters=64, \n",
    "                            kernel_size=(3,3,3),\n",
    "                            padding='same',\n",
    "                            strides=(1,1,1)\n",
    "                           )(up_depth_1_concat)\n",
    "up_depth_1_layer_1 = keras.layers.Activation('relu')(up_depth_1_layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:24.923139Z",
     "iopub.status.busy": "2022-05-04T20:23:24.922807Z",
     "iopub.status.idle": "2022-05-04T20:23:24.935623Z",
     "shell.execute_reply": "2022-05-04T20:23:24.934887Z",
     "shell.execute_reply.started": "2022-05-04T20:23:24.92311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a Conv3D up-convolution with 64 filters to your network\n",
    "up_depth_1_layer_2 = keras.layers.Conv3D(filters=64, \n",
    "                            kernel_size=(3,3,3),\n",
    "                            padding='same',\n",
    "                            strides=(1,1,1)\n",
    "                           )(up_depth_1_layer_1)\n",
    "up_depth_1_layer_2 = keras.layers.Activation('relu')(up_depth_1_layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:24.937559Z",
     "iopub.status.busy": "2022-05-04T20:23:24.937117Z",
     "iopub.status.idle": "2022-05-04T20:23:24.949759Z",
     "shell.execute_reply": "2022-05-04T20:23:24.948774Z",
     "shell.execute_reply.started": "2022-05-04T20:23:24.93751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a final Conv3D with 3 filters to your network.\n",
    "final_conv = keras.layers.Conv3D(filters=4, #4 categories \n",
    "                    kernel_size=(1,1,1),\n",
    "                    padding='valid',\n",
    "                    strides=(1,1,1)\n",
    "                    )(up_depth_1_layer_2)\n",
    "final_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:24.952084Z",
     "iopub.status.busy": "2022-05-04T20:23:24.951523Z",
     "iopub.status.idle": "2022-05-04T20:23:24.962521Z",
     "shell.execute_reply": "2022-05-04T20:23:24.961605Z",
     "shell.execute_reply.started": "2022-05-04T20:23:24.952034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a sigmoid activation to your final convolution.\n",
    "final_activation = keras.layers.Activation('sigmoid')(final_conv)\n",
    "final_activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T20:23:24.965687Z",
     "iopub.status.busy": "2022-05-04T20:23:24.965299Z",
     "iopub.status.idle": "2022-05-04T20:41:47.225675Z",
     "shell.execute_reply": "2022-05-04T20:41:47.224292Z",
     "shell.execute_reply.started": "2022-05-04T20:23:24.965649Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tensorflow.keras.models.Model(inputs=input_layer, outputs=final_activation)\n",
    "            \n",
    "model.compile(optimizer=tensorflow.keras.optimizers.Adam(lr=0.00001),\n",
    "              loss=soft_dice_loss,\n",
    "              metrics=['categorical_accuracy']\n",
    "             )\n",
    "model.summary()\n",
    "\n",
    "num_patches=len(SampleX)\n",
    "n1,n2,n3,n4 = SampleX[0].shape\n",
    "InputX = np.array(SampleX)\n",
    "InputX = InputX.reshape((num_patches,n1*n2*n3*n4))\n",
    "n1,n2,n3 = LabelY[0].shape\n",
    "InputY = np.array(LabelY)\n",
    "InputY = InputY.reshape((num_patches,n1*n2*n3))\n",
    "\n",
    "iteration = 0\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "for train, test in kfold.split(InputX, InputY):\n",
    "    trainX = np.array(SampleX)[train]\n",
    "    trainY = np.array(SampleY)[train]\n",
    "    validationX = np.array(SampleX)[test]\n",
    "    validationY = np.array(SampleY)[test]\n",
    "    model.fit(trainX,trainY,batch_size=1,epochs=100)\n",
    "    iteration = iteration + 1\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "probY = model.predict(np.array(validationX))\n",
    "probY = to_categorical(np.argmax(probY,axis=1), 4)\n",
    "predY = probY.tolist()\n",
    "\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "width=PATCH_SIZE_X\n",
    "height=PATCH_SIZE_Y\n",
    "depth=PATCH_SIZE_Z\n",
    "\n",
    "Y_True = []\n",
    "for v in validationY:\n",
    "    for x in range(0,width):\n",
    "        for y in range(0,height):\n",
    "            for z in range(0,depth):\n",
    "                for i in range(0,4):\n",
    "                    if (v[i][x][y][z] == 1):\n",
    "                        Y_True.append(i)\n",
    "                        break\n",
    "\n",
    "Y_Pred = []\n",
    "for p in predY:\n",
    "    for x in range(0,width):\n",
    "        for y in range(0,height):\n",
    "            for z in range(0,depth):\n",
    "                for i in range(0,4):\n",
    "                    if (p[x][y][z][i] == 1):\n",
    "                        Y_Pred.append(i)\n",
    "                        break\n",
    "    \n",
    "print(\"Confusion Matrix: \")\n",
    "print(multilabel_confusion_matrix(np.array(Y_True), np.array(Y_Pred)))\n",
    "print('Precision, Recall, F1 Score: ')\n",
    "print(precision_recall_fscore_support(np.array(Y_True), np.array(Y_Pred), average='weighted', labels=np.unique(Y_Pred)))\n",
    "print(\"model fit done\")\n",
    "print(model.metrics)\n",
    "\n",
    "print(\"saving model\")\n",
    "model.save('./segmentation.model')\n",
    "print(\"model save done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
