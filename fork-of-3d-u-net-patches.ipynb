{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install scikit-learn\n!pip3 install imageio\n!pip3 install IPython\n\nimport imageio\nfrom IPython.display import Image\nimport json\nimport numpy as np\nimport pandas as pd\nimport nibabel as nib\nimport matplotlib.pyplot as plt\nimport tensorflow\nfrom tensorflow import keras\nimport tensorflow.python.keras.backend as K\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom numpy import save, load\n\nfrom sklearn.metrics import (\n    average_precision_score,\n    precision_recall_curve,\n    roc_auc_score,\n    roc_curve,\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-03T08:10:04.727913Z","iopub.execute_input":"2022-07-03T08:10:04.728388Z","iopub.status.idle":"2022-07-03T08:10:26.906147Z","shell.execute_reply.started":"2022-07-03T08:10:04.728350Z","shell.execute_reply":"2022-07-03T08:10:26.905180Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.1)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.1.0)\nRequirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.20.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (3.0.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nRequirement already satisfied: imageio in /opt/conda/lib/python3.7/site-packages (2.9.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from imageio) (1.20.3)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imageio) (8.2.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nRequirement already satisfied: IPython in /opt/conda/lib/python3.7/site-packages (7.30.1)\nRequirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from IPython) (59.5.0)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from IPython) (0.18.1)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from IPython) (0.7.5)\nRequirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from IPython) (5.1.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from IPython) (3.0.24)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from IPython) (0.2.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from IPython) (5.1.0)\nRequirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from IPython) (2.10.0)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from IPython) (0.1.3)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from IPython) (4.8.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->IPython) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->IPython) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.5)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# tensorflow.keras.backend.set_floatx('float16')\n\nPATCH_SIZE_X=32\nPATCH_SIZE_Y=32\nPATCH_SIZE_Z=32\n\nSampleX = []\nSampleY = []\nLabelY = []\n\ncategory_1_count = 0\ncategory_2_count = 0\ncategory_3_count = 0\ncategory_4_count = 0\ntotal_count = 0\n\ndef weighted_categorical_crossentropy(weights):\n    weights = K.variable(weights)        \n    def loss(y_true, y_pred):\n        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n        loss = y_true * K.log(y_pred) * weights\n        loss = -K.sum(loss, -1)\n        return loss    \n    return loss\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + K.epsilon()) / (\n                K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())\n                    \ndef soft_dice_loss(y_true, y_pred):\n    print(\"y_true\")\n    print(y_true.shape)\n    print(\"y_pred\")\n    print(y_pred.shape)\n    dice_loss = 1 - (1/4 * (dice_coef(y_true[0][0],y_pred[0][0])+dice_coef(y_true[0][1],y_pred[0][1])+dice_coef(y_true[0][2],y_pred[0][2])+dice_coef(y_true[0][3],y_pred[0][3])))\n    # dice_loss = 1 - (1/3 * (dice_coef(y_true[0][0],y_pred[0][0])+dice_coef(y_true[0][1],y_pred[0][1])+dice_coef(y_true[0][2],y_pred[0][2])))\n    return dice_loss\n\ndatasets=['4channel-5/']\n\n# datasets=['model1batch1/']\n# datasets=['model1batch2/'] - best \n# datasets=['model1batch3/']\n# datasets=['model1batch4/'] \n# datasets=['modl1batch5/'] - worst\n# datasets=['model1batch6/']\n# datasets=['model1batch7/']\n# datasets = ['model1batch8/']\n# datasets = ['model1batch9/']\n# datasets=['model1batch10/']\n# datasets=['model1batch11/']\n\nfor dataset in datasets:\n    for patch_file in os.listdir('../input/'+dataset):\n        if \"nifti\" in patch_file:\n            image = np.load('../input/'+dataset+patch_file)\n            train_x = np.empty((4,PATCH_SIZE_X,PATCH_SIZE_Y,PATCH_SIZE_Z))\n            patient_id = patch_file.split('-')[1]\n            patch_num = patch_file.split('-')[2].split(\".\")[0]\n            ngtdm = 0\n            for x in range(0,PATCH_SIZE_X):\n                for y in range(0,PATCH_SIZE_Y):\n                    for z in range(0,PATCH_SIZE_Z):\n                        train_x[0][x][y][z] = np.uint8(image[0][x][y][z])\n                        train_x[1][x][y][z] = np.uint8(image[1][x][y][z])\n                        train_x[2][x][y][z] = image[2][x][y][z]\n                        train_x[3][x][y][z] = image[3][x][y][z]\n                        total_count = total_count + 1\n            SampleX.append(train_x)\n            # print('image shape')\n            # print(image.shape)\n        \n        if \"label\" in patch_file:\n            label = np.load('../input/'+dataset+patch_file)\n            train_y = np.empty((4,PATCH_SIZE_X,PATCH_SIZE_Y,PATCH_SIZE_Z))\n            patient_id = patch_file.split('-')[1]\n            patch_num = patch_file.split('-')[2].split(\".\")[0]\n            for x in range(0,PATCH_SIZE_X):\n                for y in range(0,PATCH_SIZE_Y):\n                    for z in range(0,PATCH_SIZE_Z):\n                        if label[x][y][z] == 1:\n                            train_y[0][x][y][z] = 1\n                            train_y[1][x][y][z] = 0\n                            train_y[2][x][y][z] = 0\n                            train_y[3][x][y][z] = 0\n                            category_1_count = category_1_count + 1\n                        elif label[x][y][z] == 2:\n                            # print(\"category (%d,%d,%d) 1\" % (x,y,z))\n                            train_y[0][x][y][z] = 0\n                            train_y[1][x][y][z] = 1\n                            train_y[2][x][y][z] = 0\n                            train_y[3][x][y][z] = 0\n                            category_2_count = category_2_count + 1\n                        elif label[x][y][z] == 4:\n                            # print(\"category (%d,%d,%d) 2\" % (x,y,z))\n                            train_y[0][x][y][z] = 0\n                            train_y[1][x][y][z] = 0\n                            train_y[2][x][y][z] = 0\n                            train_y[3][x][y][z] = 1\n                            category_4_count = category_4_count + 1\n                        else:\n                            # print(\"category (%d,%d,%d) 3\" % (x,y,z))\n                            train_y[0][x][y][z] = 0\n                            train_y[1][x][y][z] = 0\n                            train_y[2][x][y][z] = 1\n                            train_y[3][x][y][z] = 0\n                            category_3_count = category_3_count + 1\n            LabelY.append(label)\n            SampleY.append(train_y)\n            # print('image shape')\n            # print(image.shape)\n\n''' TBU\nweight_1 = total_count/(4*category_1_count)\nweight_2 = total_count/(4*category_2_count)\nweight_3 = total_count/(4*category_3_count)\nweight_4 = total_count/(4*category_4_count)\nprint(\"weight-1: %f\" % (weight_1))\nprint(\"weight-2: %f\" % (weight_2))\nprint(\"weight-3: %f\" % (weight_3))\nprint(\"weight-4: %f\" % (weight_4))\n'''\n\nprint(\"category_1_count: %d\" % (category_1_count))\nprint(\"category-2-count: %d\" % (category_2_count))\nprint(\"category-3-count: %d\" % (category_3_count))\nprint(\"category-4-count: %d\" % (category_4_count))\nprint(\"total count: %d\" % (total_count))\n\n''' TBU Save-Retrain-Save Loop\nweights = np.empty((4,32,32,32))\nfor x in range(0,32):\n    for y in range(0,32):\n        for z in range(0,32):\n            weights[0][x][y][z] = weight_1\n            weights[1][x][y][z] = weight_2\n            weights[2][x][y][z] = weight_3\n            weights[3][x][y][z] = weight_4\nTBU ''' \n\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.metrics import roc_curve,roc_auc_score\nfrom sklearn.metrics import multilabel_confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\n\n''' TBU Save + reload '''\nnum_patches=len(SampleX)\nn1,n2,n3,n4 = SampleX[0].shape\nprint(\"Sample: %d,%d,%d,%d\" % (n1,n2,n3,n4))\nInputX = np.array(SampleX)\nInputX = InputX.reshape((num_patches,n1*n2*n3*n4))\nprint(\"SampleY shape: \")\nprint(LabelY[0].shape)\nn1,n2,n3 = LabelY[0].shape\nprint(\"Sample: %d,%d,%d\" % (n1,n2,n3))\nInputY = np.array(LabelY)\nInputY = InputY.reshape((num_patches,n1*n2*n3))\n''' TBU Save + reload '''\n\n''' TBU: Save + Reload ''' \nmodel=tensorflow.keras.models.load_model('../input/savedmodel/segmentation.model',custom_objects={'soft_dice_loss':soft_dice_loss})\nmodel.save('./segmentation.model')\n\n# Load + Retrain\niteration = 0\nkfold = KFold(n_splits=5, shuffle=True)\nfor train, test in kfold.split(InputX, InputY):\n    trainX = np.array(SampleX)[train]\n    trainY = np.array(SampleY)[train]\n    validationX = np.array(SampleX)[test]\n    validationY = np.array(SampleY)[test]\n    \n    print(\"Model fitting iteration %d started\" % (iteration))\n    # model.fit(trainX,trainY,batch_size=1,epochs=16)\n    \n    model=tensorflow.keras.models.load_model('./segmentation.model',custom_objects={'soft_dice_loss':soft_dice_loss})\n    model.fit(np.array(trainX),np.array(trainY),batch_size=1,epochs=16,shuffle=True)\n    \n    print(\"model fit done\")\n    print(model.metrics)\n    print(\"saving model\")\n    model.save('./segmentation.model')\n    \n    print(\"model save done\")\n    print(\"Model fitting iteration %d done\" % (iteration))\n    iteration = iteration + 1\n\n    probY = model.predict(np.array(validationX))\n    print(\"probY shape: \")\n    print(probY.shape)\n    \n    probY = to_categorical(np.argmax(probY,axis=1), 4)\n    print(\"probY shape: \")\n    print(probY.shape)\n    \n    predY = probY.tolist()\n    # print(predY)\n\n    width=PATCH_SIZE_X\n    height=PATCH_SIZE_Y\n    depth=PATCH_SIZE_Z\n\n    Y_True = []\n    for v in validationY:\n        for x in range(0,width):\n            for y in range(0,height):\n                for z in range(0,depth):\n                    for i in range(0,4):\n                        if (v[i][x][y][z] == 1):\n                            Y_True.append(i)\n                            break\n\n    Y_Pred = []\n    for p in predY:\n        for x in range(0,width):\n            for y in range(0,height):\n                for z in range(0,depth):\n                    for i in range(0,4):\n                        if (p[x][y][z][i] == 1):\n                            Y_Pred.append(i)\n                            break\n    \n    print(\"Confusion Matrix: \")\n    print(multilabel_confusion_matrix(np.array(Y_True), np.array(Y_Pred)))\n\n    print('Precision, Recall, F1 Score: ')\n    # print(f1_score(np.array(Y_True), np.array(Y_Pred)))\n    print(precision_recall_fscore_support(np.array(Y_True), np.array(Y_Pred), average='weighted', labels=np.unique(Y_Pred)))\n\n    # model=tensorflow.keras.models.load_model('./segmentation.model',custom_objects={'soft_dice_loss':soft_dice_loss})\n    \n'''TBU Save-Retrain-Save Loop'''\n\n''' TBU: First iteration\n# Iterate and plot random images\nfor i in range(9):\n    img = np.empty((PATCH_SIZE_X,PATCH_SIZE_Y))\n    for y in range(0,PATCH_SIZE_X):\n        for z in range(0,PATCH_SIZE_Y):\n            img[y][z] = train_x[0][i][y][z]\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\n\n#weight_1 = total_count/(4*category_1_count)\n#weight_2 = total_count/(4*category_2_count)\n#weight_3 = total_count/(4*category_3_count)\n#weight_4 = total_count/(4*category_4_count)\n\nprint(\"Total count: %d\" % (total_count))\nprint(\"Category-1 count: %d\" % category_1_count)\nprint(\"Category-2 count: %d\" % category_2_count)\nprint(\"Category-3 count: %d\" % category_3_count)\nprint(\"Category-4 count: %d\" % category_4_count)\n\nTBU: First iteration '''","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:10:26.910425Z","iopub.execute_input":"2022-07-03T08:10:26.910674Z","iopub.status.idle":"2022-07-03T08:31:29.007969Z","shell.execute_reply.started":"2022-07-03T08:10:26.910648Z","shell.execute_reply":"2022-07-03T08:31:29.007234Z"},"trusted":true},"execution_count":138,"outputs":[{"name":"stdout","text":"category_1_count: 43168\ncategory-2-count: 285143\ncategory-3-count: 12686452\ncategory-4-count: 92437\ntotal count: 13107200\nSample: 4,32,32,32\nSampleY shape: \n(32, 32, 32)\nSample: 32,32,32\nModel fitting iteration 0 started\nEpoch 1/16\ny_true\n(1, 4, 32, 32, 32)\ny_pred\n(1, 4, 32, 32, 32)\ny_true\n(1, 4, 32, 32, 32)\ny_pred\n(1, 4, 32, 32, 32)\n320/320 [==============================] - 12s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nEpoch 2/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nEpoch 3/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nEpoch 4/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nEpoch 5/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nEpoch 6/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nEpoch 7/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nEpoch 8/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nEpoch 9/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nEpoch 10/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nEpoch 11/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nEpoch 12/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nEpoch 13/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nEpoch 14/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nEpoch 15/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nEpoch 16/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0768 - mean_io_u: 0.9672\nmodel fit done\n[<keras.metrics.Mean object at 0x7f8083f47190>, <keras.metrics.MeanIoU object at 0x7f852235fa50>]\nsaving model\nmodel save done\nModel fitting iteration 0 done\nprobY shape: \n(80, 4, 32, 32, 32)\nprobY shape: \n(80, 32, 32, 32, 4)\nConfusion Matrix: \n[[[2603803       0]\n  [  17637       0]]\n\n [[2524273       0]\n  [  97167       0]]\n\n [[      0  157257]\n  [      0 2464183]]\n\n [[2578987       0]\n  [  42453       0]]]\nPrecision, Recall, F1 Score: \n(0.940011215209961, 1.0, 0.9690781247449919, None)\nModel fitting iteration 1 started\nEpoch 1/16\ny_true\n(1, 4, 32, 32, 32)\ny_pred\n(1, 4, 32, 32, 32)\ny_true\n(1, 4, 32, 32, 32)\ny_pred\n(1, 4, 32, 32, 32)\n320/320 [==============================] - 13s 36ms/step - loss: 0.0920 - mean_io_u: 0.9584\nEpoch 2/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0920 - mean_io_u: 0.9584\nEpoch 3/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0920 - mean_io_u: 0.9584\nEpoch 4/16\n320/320 [==============================] - 11s 36ms/step - loss: 0.0920 - mean_io_u: 0.9584\nEpoch 5/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0920 - mean_io_u: 0.9584\nEpoch 6/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0920 - mean_io_u: 0.9584\nEpoch 7/16\n320/320 [==============================] - 11s 36ms/step - loss: 0.0920 - mean_io_u: 0.9584\nEpoch 8/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0920 - mean_io_u: 0.9584\nEpoch 9/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0920 - mean_io_u: 0.9584\nEpoch 10/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0920 - mean_io_u: 0.9584\nEpoch 11/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0920 - mean_io_u: 0.9584\nEpoch 12/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0920 - mean_io_u: 0.9584\nEpoch 13/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0920 - mean_io_u: 0.9584\nEpoch 14/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0920 - mean_io_u: 0.9584\nEpoch 15/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0920 - mean_io_u: 0.9584\nEpoch 16/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0920 - mean_io_u: 0.9584\nmodel fit done\n[<keras.metrics.Mean object at 0x7f85583f1190>, <keras.metrics.MeanIoU object at 0x7f85e566e910>]\nsaving model\nmodel save done\nModel fitting iteration 1 done\nprobY shape: \n(80, 4, 32, 32, 32)\nprobY shape: \n(80, 32, 32, 32, 4)\nConfusion Matrix: \n[[[2616937       0]\n  [   4503       0]]\n\n [[2551152       0]\n  [  70288       0]]\n\n [[      0   84644]\n  [      0 2536796]]\n\n [[2611587       0]\n  [   9853       0]]]\nPrecision, Recall, F1 Score: \n(0.9677108764648437, 1.0, 0.9835905142765861, None)\nModel fitting iteration 2 started\nEpoch 1/16\ny_true\n(1, 4, 32, 32, 32)\ny_pred\n(1, 4, 32, 32, 32)\ny_true\n(1, 4, 32, 32, 32)\ny_pred\n(1, 4, 32, 32, 32)\n320/320 [==============================] - 12s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nEpoch 2/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nEpoch 3/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nEpoch 4/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nEpoch 5/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nEpoch 6/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nEpoch 7/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nEpoch 8/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nEpoch 9/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nEpoch 10/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nEpoch 11/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nEpoch 12/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nEpoch 13/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nEpoch 14/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nEpoch 15/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nEpoch 16/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0858 - mean_io_u: 0.9573\nmodel fit done\n[<keras.metrics.Mean object at 0x7f80a94f7750>, <keras.metrics.MeanIoU object at 0x7f80a94f7810>]\nsaving model\nmodel save done\nModel fitting iteration 2 done\nprobY shape: \n(80, 4, 32, 32, 32)\nprobY shape: \n(80, 32, 32, 32, 4)\nConfusion Matrix: \n[[[2607130       0]\n  [  14310       0]]\n\n [[2581081       0]\n  [  40359       0]]\n\n [[      0   75942]\n  [      0 2545498]]\n\n [[2600167       0]\n  [  21273       0]]]\nPrecision, Recall, F1 Score: \n(0.9710304260253907, 1.0, 0.9853023202523428, None)\nModel fitting iteration 3 started\nEpoch 1/16\ny_true\n(1, 4, 32, 32, 32)\ny_pred\n(1, 4, 32, 32, 32)\ny_true\n(1, 4, 32, 32, 32)\ny_pred\n(1, 4, 32, 32, 32)\n320/320 [==============================] - 12s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nEpoch 2/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nEpoch 3/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nEpoch 4/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nEpoch 5/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nEpoch 6/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nEpoch 7/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nEpoch 8/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nEpoch 9/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nEpoch 10/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nEpoch 11/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nEpoch 12/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nEpoch 13/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nEpoch 14/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nEpoch 15/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nEpoch 16/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0948 - mean_io_u: 0.9555\nmodel fit done\n[<keras.metrics.Mean object at 0x7f85202ab390>, <keras.metrics.MeanIoU object at 0x7f84bc344dd0>]\nsaving model\nmodel save done\nModel fitting iteration 3 done\nprobY shape: \n(80, 4, 32, 32, 32)\nprobY shape: \n(80, 32, 32, 32, 4)\nConfusion Matrix: \n[[[2617961       0]\n  [   3479       0]]\n\n [[2575600       0]\n  [  45840       0]]\n\n [[      0   60508]\n  [      0 2560932]]\n\n [[2610251       0]\n  [  11189       0]]]\nPrecision, Recall, F1 Score: \n(0.9769180297851563, 1.0, 0.9883242654135983, None)\nModel fitting iteration 4 started\nEpoch 1/16\ny_true\n(1, 4, 32, 32, 32)\ny_pred\n(1, 4, 32, 32, 32)\ny_true\n(1, 4, 32, 32, 32)\ny_pred\n(1, 4, 32, 32, 32)\n320/320 [==============================] - 12s 35ms/step - loss: 0.0927 - mean_io_u: 0.9533\nEpoch 2/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0927 - mean_io_u: 0.9533\nEpoch 3/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0927 - mean_io_u: 0.9533\nEpoch 4/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0927 - mean_io_u: 0.9533\nEpoch 5/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0927 - mean_io_u: 0.9533\nEpoch 6/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0927 - mean_io_u: 0.9533\nEpoch 7/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0927 - mean_io_u: 0.9533\nEpoch 8/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0927 - mean_io_u: 0.9533\nEpoch 9/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0927 - mean_io_u: 0.9533\nEpoch 10/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0927 - mean_io_u: 0.9533\nEpoch 11/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0927 - mean_io_u: 0.9533\nEpoch 12/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0927 - mean_io_u: 0.9533\nEpoch 13/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0927 - mean_io_u: 0.9533\nEpoch 14/16\n320/320 [==============================] - 11s 36ms/step - loss: 0.0927 - mean_io_u: 0.9533\nEpoch 15/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0927 - mean_io_u: 0.9533\nEpoch 16/16\n320/320 [==============================] - 11s 35ms/step - loss: 0.0927 - mean_io_u: 0.9533\nmodel fit done\n[<keras.metrics.Mean object at 0x7f849ecc4bd0>, <keras.metrics.MeanIoU object at 0x7f8083dc88d0>]\nsaving model\nmodel save done\nModel fitting iteration 4 done\nprobY shape: \n(80, 4, 32, 32, 32)\nprobY shape: \n(80, 32, 32, 32, 4)\nConfusion Matrix: \n[[[2618201       0]\n  [   3239       0]]\n\n [[2589951       0]\n  [  31489       0]]\n\n [[      0   42397]\n  [      0 2579043]]\n\n [[2613771       0]\n  [   7669       0]]]\nPrecision, Recall, F1 Score: \n(0.9838268280029298, 1.0, 0.9918474880121712, None)\n","output_type":"stream"},{"execution_count":138,"output_type":"execute_result","data":{"text/plain":"' TBU: First iteration\\n# Iterate and plot random images\\nfor i in range(9):\\n    img = np.empty((PATCH_SIZE_X,PATCH_SIZE_Y))\\n    for y in range(0,PATCH_SIZE_X):\\n        for z in range(0,PATCH_SIZE_Y):\\n            img[y][z] = train_x[0][i][y][z]\\n    plt.subplot(3, 3, i + 1)\\n    plt.imshow(img, cmap=\\'gray\\')\\n    plt.axis(\\'off\\')\\n\\n#weight_1 = total_count/(4*category_1_count)\\n#weight_2 = total_count/(4*category_2_count)\\n#weight_3 = total_count/(4*category_3_count)\\n#weight_4 = total_count/(4*category_4_count)\\n\\nprint(\"Total count: %d\" % (total_count))\\nprint(\"Category-1 count: %d\" % category_1_count)\\nprint(\"Category-2 count: %d\" % category_2_count)\\nprint(\"Category-3 count: %d\" % category_3_count)\\nprint(\"Category-4 count: %d\" % category_4_count)\\n\\nTBU: First iteration '"},"metadata":{}}]},{"cell_type":"code","source":"''' TBU: First iteration\nimport tensorflow\nfrom tensorflow import keras\nfrom keras import backend as K\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Conv3D, MaxPooling3D, UpSampling3D, Activation, BatchNormalization, PReLU, Conv3DTranspose\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers.merge import concatenate\n\nkeras.backend.set_image_data_format(\"channels_first\")\nTBU: First iteration '''","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:31:29.009230Z","iopub.execute_input":"2022-07-03T08:31:29.009571Z","iopub.status.idle":"2022-07-03T08:31:29.018330Z","shell.execute_reply.started":"2022-07-03T08:31:29.009530Z","shell.execute_reply":"2022-07-03T08:31:29.017467Z"},"trusted":true},"execution_count":139,"outputs":[{"execution_count":139,"output_type":"execute_result","data":{"text/plain":"' TBU: First iteration\\nimport tensorflow\\nfrom tensorflow import keras\\nfrom keras import backend as K\\nfrom tensorflow.keras.layers import Input\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.layers import Conv3D, MaxPooling3D, UpSampling3D, Activation, BatchNormalization, PReLU, Conv3DTranspose\\nfrom tensorflow.keras.optimizers import Adam\\nfrom keras.layers.merge import concatenate\\n\\nkeras.backend.set_image_data_format(\"channels_first\")\\nTBU: First iteration '"},"metadata":{}}]},{"cell_type":"code","source":"''' TBU: First iteration\ninput_layer = Input(shape=(4,32,32,32))\n# input_layer = Input(shape=(3,32,32,32))\n# input_layer = Input(shape=(1,32,32,32))\ninput_layer\n\ndown_depth_0_layer_0 = keras.layers.Conv3D(filters=32, \n                              kernel_size=(3,3,3),\n                              padding='same',\n                              strides=(1,1,1),\n                              data_format='channels_first',\n                              input_shape=(4,32,32,32)\n                              # input_shape=(1,32,32,32)\n                              )(input_layer)\n\ndown_depth_0_layer_0\nTBU: First iteration '''","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:31:29.021504Z","iopub.execute_input":"2022-07-03T08:31:29.022412Z","iopub.status.idle":"2022-07-03T08:31:29.032164Z","shell.execute_reply.started":"2022-07-03T08:31:29.022200Z","shell.execute_reply":"2022-07-03T08:31:29.031348Z"},"trusted":true},"execution_count":140,"outputs":[{"execution_count":140,"output_type":"execute_result","data":{"text/plain":"\" TBU: First iteration\\ninput_layer = Input(shape=(4,32,32,32))\\n# input_layer = Input(shape=(3,32,32,32))\\n# input_layer = Input(shape=(1,32,32,32))\\ninput_layer\\n\\ndown_depth_0_layer_0 = keras.layers.Conv3D(filters=32, \\n                              kernel_size=(3,3,3),\\n                              padding='same',\\n                              strides=(1,1,1),\\n                              data_format='channels_first',\\n                              input_shape=(4,32,32,32)\\n                              # input_shape=(1,32,32,32)\\n                              )(input_layer)\\n\\ndown_depth_0_layer_0\\nTBU: First iteration \""},"metadata":{}}]},{"cell_type":"code","source":"''' TBU: First iteration\ndown_depth_0_layer_0 = Activation('relu')(down_depth_0_layer_0)\ndown_depth_0_layer_0\nTBU First iteration '''","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:31:29.033472Z","iopub.execute_input":"2022-07-03T08:31:29.034369Z","iopub.status.idle":"2022-07-03T08:31:29.043271Z","shell.execute_reply.started":"2022-07-03T08:31:29.034321Z","shell.execute_reply":"2022-07-03T08:31:29.042558Z"},"trusted":true},"execution_count":141,"outputs":[{"execution_count":141,"output_type":"execute_result","data":{"text/plain":"\" TBU: First iteration\\ndown_depth_0_layer_0 = Activation('relu')(down_depth_0_layer_0)\\ndown_depth_0_layer_0\\nTBU First iteration \""},"metadata":{}}]},{"cell_type":"code","source":"''' TBU: First iteration\n# Create a Conv3D layer with 64 filters and add relu activation\ndown_depth_0_layer_1 = Conv3D(filters=64, \n                kernel_size=(3,3,3),\n                padding='same',\n                strides=(1,1,1)\n               )(down_depth_0_layer_0)\ndown_depth_0_layer_1 = Activation('relu')(down_depth_0_layer_1)\ndown_depth_0_layer_1\nTBU First iteration '''\n","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:31:29.045458Z","iopub.execute_input":"2022-07-03T08:31:29.045740Z","iopub.status.idle":"2022-07-03T08:31:29.057064Z","shell.execute_reply.started":"2022-07-03T08:31:29.045707Z","shell.execute_reply":"2022-07-03T08:31:29.056289Z"},"trusted":true},"execution_count":142,"outputs":[{"execution_count":142,"output_type":"execute_result","data":{"text/plain":"\" TBU: First iteration\\n# Create a Conv3D layer with 64 filters and add relu activation\\ndown_depth_0_layer_1 = Conv3D(filters=64, \\n                kernel_size=(3,3,3),\\n                padding='same',\\n                strides=(1,1,1)\\n               )(down_depth_0_layer_0)\\ndown_depth_0_layer_1 = Activation('relu')(down_depth_0_layer_1)\\ndown_depth_0_layer_1\\nTBU First iteration \""},"metadata":{}}]},{"cell_type":"code","source":"''' TBU: First iteration\n# Define a max pooling layer\n# down_depth_0_layer_pool = MaxPooling3D(pool_size=(2,2,2))(down_depth_0_layer_1)\ndown_depth_0_layer_pool = MaxPooling3D(pool_size=(2,2,2),data_format='channels_first')(down_depth_0_layer_1)\ndown_depth_0_layer_pool\nTBU: First iteration '''","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:31:29.058390Z","iopub.execute_input":"2022-07-03T08:31:29.058662Z","iopub.status.idle":"2022-07-03T08:31:29.068300Z","shell.execute_reply.started":"2022-07-03T08:31:29.058614Z","shell.execute_reply":"2022-07-03T08:31:29.067601Z"},"trusted":true},"execution_count":143,"outputs":[{"execution_count":143,"output_type":"execute_result","data":{"text/plain":"\" TBU: First iteration\\n# Define a max pooling layer\\n# down_depth_0_layer_pool = MaxPooling3D(pool_size=(2,2,2))(down_depth_0_layer_1)\\ndown_depth_0_layer_pool = MaxPooling3D(pool_size=(2,2,2),data_format='channels_first')(down_depth_0_layer_1)\\ndown_depth_0_layer_pool\\nTBU: First iteration \""},"metadata":{}}]},{"cell_type":"code","source":"''' TBU: First iteration\n# Add a Conv3D layer to your network with relu activation\ndown_depth_1_layer_0 = Conv3D(filters=64, \n                kernel_size=(3,3,3),\n                padding='same',\n                strides=(1,1,1)\n               )(down_depth_0_layer_pool)\ndown_depth_1_layer_0 = Activation('relu')(down_depth_1_layer_0)\ndown_depth_1_layer_0\nTBU: First iteration '''","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:31:29.069746Z","iopub.execute_input":"2022-07-03T08:31:29.070226Z","iopub.status.idle":"2022-07-03T08:31:29.083318Z","shell.execute_reply.started":"2022-07-03T08:31:29.070171Z","shell.execute_reply":"2022-07-03T08:31:29.082588Z"},"trusted":true},"execution_count":144,"outputs":[{"execution_count":144,"output_type":"execute_result","data":{"text/plain":"\" TBU: First iteration\\n# Add a Conv3D layer to your network with relu activation\\ndown_depth_1_layer_0 = Conv3D(filters=64, \\n                kernel_size=(3,3,3),\\n                padding='same',\\n                strides=(1,1,1)\\n               )(down_depth_0_layer_pool)\\ndown_depth_1_layer_0 = Activation('relu')(down_depth_1_layer_0)\\ndown_depth_1_layer_0\\nTBU: First iteration \""},"metadata":{}}]},{"cell_type":"code","source":"''' TBU: First iteration\n# Add another Conv3D with 128 filters to your network.\ndown_depth_1_layer_1 = Conv3D(filters=128, \n                kernel_size=(3,3,3),\n                padding='same',\n                strides=(1,1,1)\n               )(down_depth_1_layer_0)\ndown_depth_1_layer_1 = Activation('relu')(down_depth_1_layer_1)\ndown_depth_1_layer_1\nTBU: First iteration '''","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:31:29.084368Z","iopub.execute_input":"2022-07-03T08:31:29.087450Z","iopub.status.idle":"2022-07-03T08:31:29.097449Z","shell.execute_reply.started":"2022-07-03T08:31:29.087357Z","shell.execute_reply":"2022-07-03T08:31:29.096480Z"},"trusted":true},"execution_count":145,"outputs":[{"execution_count":145,"output_type":"execute_result","data":{"text/plain":"\" TBU: First iteration\\n# Add another Conv3D with 128 filters to your network.\\ndown_depth_1_layer_1 = Conv3D(filters=128, \\n                kernel_size=(3,3,3),\\n                padding='same',\\n                strides=(1,1,1)\\n               )(down_depth_1_layer_0)\\ndown_depth_1_layer_1 = Activation('relu')(down_depth_1_layer_1)\\ndown_depth_1_layer_1\\nTBU: First iteration \""},"metadata":{}}]},{"cell_type":"code","source":"''' TBU: First iteration\n# Add an upsampling operation to your network\nup_depth_0_layer_0 = UpSampling3D(size=(2,2,2))(down_depth_1_layer_1)\nup_depth_0_layer_0\nTBU: First iteration '''","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:31:29.100274Z","iopub.execute_input":"2022-07-03T08:31:29.100540Z","iopub.status.idle":"2022-07-03T08:31:29.108481Z","shell.execute_reply.started":"2022-07-03T08:31:29.100497Z","shell.execute_reply":"2022-07-03T08:31:29.107813Z"},"trusted":true},"execution_count":146,"outputs":[{"execution_count":146,"output_type":"execute_result","data":{"text/plain":"' TBU: First iteration\\n# Add an upsampling operation to your network\\nup_depth_0_layer_0 = UpSampling3D(size=(2,2,2))(down_depth_1_layer_1)\\nup_depth_0_layer_0\\nTBU: First iteration '"},"metadata":{}}]},{"cell_type":"code","source":"''' TBU: First iteration\n# Print the shape of layers to concatenate\nprint(up_depth_0_layer_0)\nprint()\nprint(down_depth_0_layer_1)\nTBU: First iteration '''","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:31:29.110016Z","iopub.execute_input":"2022-07-03T08:31:29.110420Z","iopub.status.idle":"2022-07-03T08:31:29.121753Z","shell.execute_reply.started":"2022-07-03T08:31:29.110377Z","shell.execute_reply":"2022-07-03T08:31:29.121076Z"},"trusted":true},"execution_count":147,"outputs":[{"execution_count":147,"output_type":"execute_result","data":{"text/plain":"' TBU: First iteration\\n# Print the shape of layers to concatenate\\nprint(up_depth_0_layer_0)\\nprint()\\nprint(down_depth_0_layer_1)\\nTBU: First iteration '"},"metadata":{}}]},{"cell_type":"code","source":"''' TBU: First iteration\n# Add a concatenation along axis 1\nup_depth_1_concat = concatenate([up_depth_0_layer_0,\n                                 down_depth_0_layer_1],\n                                axis=1)\nup_depth_1_concat\nTBU: First iteration '''","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:31:29.123336Z","iopub.execute_input":"2022-07-03T08:31:29.123906Z","iopub.status.idle":"2022-07-03T08:31:29.132661Z","shell.execute_reply.started":"2022-07-03T08:31:29.123867Z","shell.execute_reply":"2022-07-03T08:31:29.132002Z"},"trusted":true},"execution_count":148,"outputs":[{"execution_count":148,"output_type":"execute_result","data":{"text/plain":"' TBU: First iteration\\n# Add a concatenation along axis 1\\nup_depth_1_concat = concatenate([up_depth_0_layer_0,\\n                                 down_depth_0_layer_1],\\n                                axis=1)\\nup_depth_1_concat\\nTBU: First iteration '"},"metadata":{}}]},{"cell_type":"code","source":"''' TBU: First iteration\ndown_depth_0_layer_1\n# Add a Conv3D up-convolution with 64 filters to your network\nup_depth_1_layer_1 = keras.layers.Conv3D(filters=64, \n                            kernel_size=(3,3,3),\n                            padding='same',\n                            strides=(1,1,1)\n                           )(up_depth_1_concat)\nup_depth_1_layer_1 = keras.layers.Activation('relu')(up_depth_1_layer_1)\nup_depth_1_layer_1\nTBU: First iteration '''","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:31:29.134227Z","iopub.execute_input":"2022-07-03T08:31:29.134732Z","iopub.status.idle":"2022-07-03T08:31:29.145471Z","shell.execute_reply.started":"2022-07-03T08:31:29.134695Z","shell.execute_reply":"2022-07-03T08:31:29.144701Z"},"trusted":true},"execution_count":149,"outputs":[{"execution_count":149,"output_type":"execute_result","data":{"text/plain":"\" TBU: First iteration\\ndown_depth_0_layer_1\\n# Add a Conv3D up-convolution with 64 filters to your network\\nup_depth_1_layer_1 = keras.layers.Conv3D(filters=64, \\n                            kernel_size=(3,3,3),\\n                            padding='same',\\n                            strides=(1,1,1)\\n                           )(up_depth_1_concat)\\nup_depth_1_layer_1 = keras.layers.Activation('relu')(up_depth_1_layer_1)\\nup_depth_1_layer_1\\nTBU: First iteration \""},"metadata":{}}]},{"cell_type":"code","source":"''' TBU: First iteration\n# Add a Conv3D up-convolution with 64 filters to your network\nup_depth_1_layer_2 = keras.layers.Conv3D(filters=64, \n                            kernel_size=(3,3,3),\n                            padding='same',\n                            strides=(1,1,1)\n                           )(up_depth_1_layer_1)\nup_depth_1_layer_2 = keras.layers.Activation('relu')(up_depth_1_layer_2)\nup_depth_1_layer_2\nTBU: First iteration '''","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:31:29.146795Z","iopub.execute_input":"2022-07-03T08:31:29.147264Z","iopub.status.idle":"2022-07-03T08:31:29.159063Z","shell.execute_reply.started":"2022-07-03T08:31:29.147228Z","shell.execute_reply":"2022-07-03T08:31:29.158304Z"},"trusted":true},"execution_count":150,"outputs":[{"execution_count":150,"output_type":"execute_result","data":{"text/plain":"\" TBU: First iteration\\n# Add a Conv3D up-convolution with 64 filters to your network\\nup_depth_1_layer_2 = keras.layers.Conv3D(filters=64, \\n                            kernel_size=(3,3,3),\\n                            padding='same',\\n                            strides=(1,1,1)\\n                           )(up_depth_1_layer_1)\\nup_depth_1_layer_2 = keras.layers.Activation('relu')(up_depth_1_layer_2)\\nup_depth_1_layer_2\\nTBU: First iteration \""},"metadata":{}}]},{"cell_type":"code","source":"''' TBU: First iteration\n# Add a final Conv3D with 3 filters to your network.\nfinal_conv = keras.layers.Conv3D(filters=4, #4 categories \n                    kernel_size=(1,1,1),\n                    padding='valid',\n                    strides=(1,1,1)\n                    )(up_depth_1_layer_2)\nfinal_conv\nTBU: First iteration '''","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:31:29.161608Z","iopub.execute_input":"2022-07-03T08:31:29.162087Z","iopub.status.idle":"2022-07-03T08:31:29.176359Z","shell.execute_reply.started":"2022-07-03T08:31:29.162046Z","shell.execute_reply":"2022-07-03T08:31:29.175691Z"},"trusted":true},"execution_count":151,"outputs":[{"execution_count":151,"output_type":"execute_result","data":{"text/plain":"\" TBU: First iteration\\n# Add a final Conv3D with 3 filters to your network.\\nfinal_conv = keras.layers.Conv3D(filters=4, #4 categories \\n                    kernel_size=(1,1,1),\\n                    padding='valid',\\n                    strides=(1,1,1)\\n                    )(up_depth_1_layer_2)\\nfinal_conv\\nTBU: First iteration \""},"metadata":{}}]},{"cell_type":"code","source":"''' TBU: First iteration\n# Add a sigmoid activation to your final convolution.\nfinal_activation = keras.layers.Activation('sigmoid')(final_conv)\nfinal_activation\nTBU: First iteration '''","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:31:29.178489Z","iopub.execute_input":"2022-07-03T08:31:29.179559Z","iopub.status.idle":"2022-07-03T08:31:29.187647Z","shell.execute_reply.started":"2022-07-03T08:31:29.179513Z","shell.execute_reply":"2022-07-03T08:31:29.186742Z"},"trusted":true},"execution_count":152,"outputs":[{"execution_count":152,"output_type":"execute_result","data":{"text/plain":"\" TBU: First iteration\\n# Add a sigmoid activation to your final convolution.\\nfinal_activation = keras.layers.Activation('sigmoid')(final_conv)\\nfinal_activation\\nTBU: First iteration \""},"metadata":{}}]},{"cell_type":"code","source":"''' TBU: First iteration\ndef weighted_categorical_crossentropy(weights):\n    weights = K.variable(weights)        \n    def loss(y_true, y_pred):\n        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n        loss = y_true * K.log(y_pred) * weights\n        loss = -K.sum(loss, -1)\n        return loss\n    return loss\n\ndef plot_roc_curve(fpr,tpr): \n    plt.plot(fpr,tpr) \n    plt.axis([0,1,0,1]) \n    plt.xlabel('False Positive Rate') \n    plt.ylabel('True Positive Rate') \n    plt.show()    \n  \nmodel = tensorflow.keras.models.Model(inputs=input_layer, outputs=final_activation)\n\nweights = np.empty((4,32,32,32))\nfor x in range(0,32):\n    for y in range(0,32):\n        for z in range(0,32):\n            weights[0][x][y][z] = weight_1\n            weights[1][x][y][z] = weight_2\n            weights[2][x][y][z] = weight_3\n            weights[3][x][y][z] = weight_4\n\nmodel.compile(optimizer=tensorflow.keras.optimizers.Adam(lr=0.00001),\n              # loss='categorical_crossentropy',\n              # loss=weighted_categorical_crossentropy(weights),\n              loss=soft_dice_loss,\n              # metrics=['categorical_accuracy']\n              metrics=[tensorflow.keras.metrics.MeanIoU(num_classes=4)]\n             )\nmodel.summary()\n\nnum_patches=len(SampleX)\nprint(\"num_patches: %d\" % num_patches)\nn1,n2,n3,n4 = SampleX[0].shape\nprint(\"Sample: %d,%d,%d,%d\" % (n1,n2,n3,n4))\nInputX = np.array(SampleX)\nInputX = InputX.reshape((num_patches,n1*n2*n3*n4))\nprint(\"SampleY shape: \")\nprint(\"num patches: %d\" % len(LabelY))\nprint(LabelY[0].shape)\nn1,n2,n3 = LabelY[0].shape\nprint(\"Sample: %d,%d,%d\" % (n1,n2,n3))\nInputY = np.array(LabelY)\nInputY = InputY.reshape((num_patches,n1*n2*n3))\n\niteration = 0\nkfold = KFold(n_splits=5, shuffle=True)\nfor train, test in kfold.split(InputX, InputY):\n    trainX = np.array(SampleX)[train]\n    trainY = np.array(SampleY)[train]\n    validationX = np.array(SampleX)[test]\n    validationY = np.array(SampleY)[test]\n    print(\"Model fitting iteration %d started\" % (iteration))\n    model.fit(trainX,trainY,batch_size=1,epochs=16)\n    # model.fit(trainX,trainY,batch_size=1,epochs=1)\n    print(\"Model fitting iteration %d done\" % (iteration))\n    iteration = iteration + 1\n\nfrom keras.utils.np_utils import to_categorical\n\nprobY = model.predict(np.array(validationX))\nprint(\"probY shape: \")\nprint(probY.shape)\nprobY = to_categorical(np.argmax(probY,axis=1), 4)\nprint(\"probY shape: \")\nprint(probY.shape)\npredY = probY.tolist()\n# print(predY)\n\nfrom sklearn.metrics import roc_curve,roc_auc_score\nfrom sklearn.metrics import multilabel_confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\n\nwidth=PATCH_SIZE_X\nheight=PATCH_SIZE_Y\ndepth=PATCH_SIZE_Z\n\nY_True = []\nfor v in validationY:\n    for x in range(0,width):\n        for y in range(0,height):\n            for z in range(0,depth):\n                for i in range(0,4):\n                    if (v[i][x][y][z] == 1):\n                        Y_True.append(i)\n                        break\n\nY_Pred = []\nfor p in predY:\n    for x in range(0,width):\n        for y in range(0,height):\n            for z in range(0,depth):\n                for i in range(0,4):\n                    if (p[x][y][z][i] == 1):\n                        Y_Pred.append(i)\n                        # if (i != 0):\n                        #    print(\"Pred: %d,%d,%d: label: %d\" % (x,y,z,i))\n                        break\n    \nprint(\"Confusion Matrix: \")\nprint(multilabel_confusion_matrix(np.array(Y_True), np.array(Y_Pred)))\n\nprint('Precision, Recall, F1 Score: ')\n# print(f1_score(np.array(Y_True), np.array(Y_Pred)))\nprint(precision_recall_fscore_support(np.array(Y_True), np.array(Y_Pred), average='weighted', labels=np.unique(Y_Pred)))\nTBU '''\n\n'''\nTestX = []\nPatchFileNames = []\ndef load_dicom_files():\n    datasets=['../input/segmentation-model-input-2/']\n    # datasets=['../input/dicom4/']\n    # datasets=['../input/dicom1/', '../input/dicoom2/', '../input/dicom3', '../input/dicom4']\n    for dataset in datasets:\n        for patch_file in os.listdir(dataset):\n            # TBU if \"dicom\" in patch_file:\n            if \"nifti\" in patch_file:\n                PatchFileNames.append(patch_file)\n                image = np.load(dataset+patch_file)\n                test_x = np.empty((3,PATCH_SIZE_X,PATCH_SIZE_Y,PATCH_SIZE_Z))\n                patient_id = patch_file.split('-')[1]\n                patch_num = patch_file.split('-')[2].split(\".\")[0]\n                ngtdm = 0\n                for x in range(0,PATCH_SIZE_X):\n                    for y in range(0,PATCH_SIZE_Y):\n                        for z in range(0,PATCH_SIZE_Z):\n                            test_x[0][x][y][z] = np.uint8(image[0][x][y][z])\n                            test_x[1][x][y][z] = np.uint8(image[1][x][y][z])\n                            test_x[2][x][y][z] = image[3][x][y][z]\n                TestX.append(test_x)\n\nload_dicom_files()\nprobY = model.predict(np.array(TestX))\nprint(\"probY shape: \")\nprint(probY.shape)\n\nprobY = to_categorical(np.argmax(probY,axis=1), 4)\nprint(\"probY shape: \")\nprint(probY.shape)\n\n# predY = probY.tolist()\n# print(predY)\n\npatch_count = 0\nfor y_pred in probY:\n    width,height,depth,channels = y_pred.shape\n    valid_patch_found = False\n    count_pixels_with_label_1 = 0\n    count_pixels_with_label_2 = 0\n    count_pixels_with_label_3 = 0\n    count_pixels_with_label_4 = 0\n    for x in range(0,width):\n        for y in range(0,height):\n            for z in range(0,depth):\n                if y_pred[x][y][z][0] == 1:\n                    count_pixels_with_label_1 = count_pixels_with_label_1 + 1\n                elif y_pred[x][y][z][1] == 1:\n                    count_pixels_with_label_2 = count_pixels_with_label_2 + 1 \n                elif y_pred[x][y][z][2] == 1:\n                    count_pixels_with_label_3 = count_pixels_with_label_3 + 1\n                else:\n                    count_pixels_with_label_4 = count_pixels_with_label_4 + 1\n    print(\"Count of non-background pixels %d\" % (count_pixels_with_label_1+count_pixels_with_label_2+count_pixels_with_label_4))\n    # TBU: if ((count_pixels_with_label_1+count_pixels_with_label_2+count_pixels_with_label_4)/(32*32*32) > .5):\n    valid_patch = np.empty((32,32,32,2))\n    for x in range(0,32):\n        for y in range(0,32):\n            for z in range(0,32):\n                valid_patch[x][y][z][0] = TestX[patch_count][0][x][y][z]\n                if y_pred[x][y][z][0] == 1:\n                    valid_patch[x][y][z][1] = 1\n                elif y_pred[x][y][z][1] == 1:\n                    valid_patch[x][y][z][1] = 2\n                elif y_pred[x][y][z][3] == 1:\n                    valid_patch[x][y][z][1] = 4\n                else:\n                    valid_patch[x][y][z][1] = 3\n    # output_file = './' + 'densenet-input-' + PatchFileNames[patch_count]\n    output_file = './' + 'segmented-image-' + PatchFileNames[patch_count]\n    print('Picked %d, %s for training densenet' % (patch_count, output_file))\n    save(output_file, valid_patch)\n    patch_count = patch_count + 1\n\n# filter only slices with labels 1,2,4\n# Create a patch array with 32x32x32 by 2 (greyscale+label)\n# Save these slices to output directory\n\n# Download them \n# Create label patch array with 32x32x32 by 1 for MGMT label\n# Train densenet model \n# get confusion matrix\n# get f1 score, precision, recall\n\nfpr = dict()\ntpr = dict()\nfor i in range(0,4):\n    Vy = []\n    # print(\"vy: \")\n    for v in validationY:\n        width, height, depth = v[i].shape\n        vy = np.empty((width,height,depth))\n        for x in range(0,width):\n            for y in range(0,height):\n                for z in range(0,depth):\n                    vy[x][y][z] = v[i][x][y][z]\n        flattened_vy = vy.flatten()\n        # print(flattened_vy)\n        Vy.extend(flattened_vy.tolist())\n    # print('Vy: ')\n    # print(Vy)\n    Py = []\n    # print(\"py: \")\n    for p in predY:\n        width = height = depth = len(p[i])\n        print('width %d, height %d, depth %d' % (width, height, depth))\n        py = np.empty((width,height,depth))\n        for x in range(0,width):\n            for y in range(0,height):\n                for z in range(0,depth):\n                    # print('i %d, x %d, y %d, z %d' % (i, x, y, z))\n                    py[x][y][z] = p[x][y][z][i]\n        flattened_py = py.flatten()\n        # print(flattened_py)\n        Py.extend(flattened_py.tolist())\n    # print('Py: ')\n    # print(Py)\n    # print(\"V: class %d\" % i)\n    for v in Vy:\n        if ((v != 0) and (v != 1)):\n            # print(\"v: %d\" % v)\n            break\n    # print(\"P: class %d\" % i)\n    for p in Py:\n        if ((p != 0) and (p != 1)):\n            # print(\"p: %d\" % p)\n            break\n    fpr[i], tpr[i], _ = roc_curve(np.array(Vy), np.array(Py))\n\nfor i in range(0,4):\n    plot_roc_curve(fpr[i],tpr[i]) \n\nprint(\"model fit done\")\nprint(model.metrics)\nprint(\"saving model\")\nmodel.save('./segmentation.model')\nprint(\"model save done\") '''","metadata":{"execution":{"iopub.status.busy":"2022-07-03T08:31:29.190474Z","iopub.execute_input":"2022-07-03T08:31:29.190805Z","iopub.status.idle":"2022-07-03T08:31:29.208299Z","shell.execute_reply.started":"2022-07-03T08:31:29.190775Z","shell.execute_reply":"2022-07-03T08:31:29.207655Z"},"trusted":true},"execution_count":153,"outputs":[{"execution_count":153,"output_type":"execute_result","data":{"text/plain":"'\\nTestX = []\\nPatchFileNames = []\\ndef load_dicom_files():\\n    datasets=[\\'../input/segmentation-model-input-2/\\']\\n    # datasets=[\\'../input/dicom4/\\']\\n    # datasets=[\\'../input/dicom1/\\', \\'../input/dicoom2/\\', \\'../input/dicom3\\', \\'../input/dicom4\\']\\n    for dataset in datasets:\\n        for patch_file in os.listdir(dataset):\\n            # TBU if \"dicom\" in patch_file:\\n            if \"nifti\" in patch_file:\\n                PatchFileNames.append(patch_file)\\n                image = np.load(dataset+patch_file)\\n                test_x = np.empty((3,PATCH_SIZE_X,PATCH_SIZE_Y,PATCH_SIZE_Z))\\n                patient_id = patch_file.split(\\'-\\')[1]\\n                patch_num = patch_file.split(\\'-\\')[2].split(\".\")[0]\\n                ngtdm = 0\\n                for x in range(0,PATCH_SIZE_X):\\n                    for y in range(0,PATCH_SIZE_Y):\\n                        for z in range(0,PATCH_SIZE_Z):\\n                            test_x[0][x][y][z] = np.uint8(image[0][x][y][z])\\n                            test_x[1][x][y][z] = np.uint8(image[1][x][y][z])\\n                            test_x[2][x][y][z] = image[3][x][y][z]\\n                TestX.append(test_x)\\n\\nload_dicom_files()\\nprobY = model.predict(np.array(TestX))\\nprint(\"probY shape: \")\\nprint(probY.shape)\\n\\nprobY = to_categorical(np.argmax(probY,axis=1), 4)\\nprint(\"probY shape: \")\\nprint(probY.shape)\\n\\n# predY = probY.tolist()\\n# print(predY)\\n\\npatch_count = 0\\nfor y_pred in probY:\\n    width,height,depth,channels = y_pred.shape\\n    valid_patch_found = False\\n    count_pixels_with_label_1 = 0\\n    count_pixels_with_label_2 = 0\\n    count_pixels_with_label_3 = 0\\n    count_pixels_with_label_4 = 0\\n    for x in range(0,width):\\n        for y in range(0,height):\\n            for z in range(0,depth):\\n                if y_pred[x][y][z][0] == 1:\\n                    count_pixels_with_label_1 = count_pixels_with_label_1 + 1\\n                elif y_pred[x][y][z][1] == 1:\\n                    count_pixels_with_label_2 = count_pixels_with_label_2 + 1 \\n                elif y_pred[x][y][z][2] == 1:\\n                    count_pixels_with_label_3 = count_pixels_with_label_3 + 1\\n                else:\\n                    count_pixels_with_label_4 = count_pixels_with_label_4 + 1\\n    print(\"Count of non-background pixels %d\" % (count_pixels_with_label_1+count_pixels_with_label_2+count_pixels_with_label_4))\\n    # TBU: if ((count_pixels_with_label_1+count_pixels_with_label_2+count_pixels_with_label_4)/(32*32*32) > .5):\\n    valid_patch = np.empty((32,32,32,2))\\n    for x in range(0,32):\\n        for y in range(0,32):\\n            for z in range(0,32):\\n                valid_patch[x][y][z][0] = TestX[patch_count][0][x][y][z]\\n                if y_pred[x][y][z][0] == 1:\\n                    valid_patch[x][y][z][1] = 1\\n                elif y_pred[x][y][z][1] == 1:\\n                    valid_patch[x][y][z][1] = 2\\n                elif y_pred[x][y][z][3] == 1:\\n                    valid_patch[x][y][z][1] = 4\\n                else:\\n                    valid_patch[x][y][z][1] = 3\\n    # output_file = \\'./\\' + \\'densenet-input-\\' + PatchFileNames[patch_count]\\n    output_file = \\'./\\' + \\'segmented-image-\\' + PatchFileNames[patch_count]\\n    print(\\'Picked %d, %s for training densenet\\' % (patch_count, output_file))\\n    save(output_file, valid_patch)\\n    patch_count = patch_count + 1\\n\\n# filter only slices with labels 1,2,4\\n# Create a patch array with 32x32x32 by 2 (greyscale+label)\\n# Save these slices to output directory\\n\\n# Download them \\n# Create label patch array with 32x32x32 by 1 for MGMT label\\n# Train densenet model \\n# get confusion matrix\\n# get f1 score, precision, recall\\n\\nfpr = dict()\\ntpr = dict()\\nfor i in range(0,4):\\n    Vy = []\\n    # print(\"vy: \")\\n    for v in validationY:\\n        width, height, depth = v[i].shape\\n        vy = np.empty((width,height,depth))\\n        for x in range(0,width):\\n            for y in range(0,height):\\n                for z in range(0,depth):\\n                    vy[x][y][z] = v[i][x][y][z]\\n        flattened_vy = vy.flatten()\\n        # print(flattened_vy)\\n        Vy.extend(flattened_vy.tolist())\\n    # print(\\'Vy: \\')\\n    # print(Vy)\\n    Py = []\\n    # print(\"py: \")\\n    for p in predY:\\n        width = height = depth = len(p[i])\\n        print(\\'width %d, height %d, depth %d\\' % (width, height, depth))\\n        py = np.empty((width,height,depth))\\n        for x in range(0,width):\\n            for y in range(0,height):\\n                for z in range(0,depth):\\n                    # print(\\'i %d, x %d, y %d, z %d\\' % (i, x, y, z))\\n                    py[x][y][z] = p[x][y][z][i]\\n        flattened_py = py.flatten()\\n        # print(flattened_py)\\n        Py.extend(flattened_py.tolist())\\n    # print(\\'Py: \\')\\n    # print(Py)\\n    # print(\"V: class %d\" % i)\\n    for v in Vy:\\n        if ((v != 0) and (v != 1)):\\n            # print(\"v: %d\" % v)\\n            break\\n    # print(\"P: class %d\" % i)\\n    for p in Py:\\n        if ((p != 0) and (p != 1)):\\n            # print(\"p: %d\" % p)\\n            break\\n    fpr[i], tpr[i], _ = roc_curve(np.array(Vy), np.array(Py))\\n\\nfor i in range(0,4):\\n    plot_roc_curve(fpr[i],tpr[i]) \\n\\nprint(\"model fit done\")\\nprint(model.metrics)\\nprint(\"saving model\")\\nmodel.save(\\'./segmentation.model\\')\\nprint(\"model save done\") '"},"metadata":{}}]}]}